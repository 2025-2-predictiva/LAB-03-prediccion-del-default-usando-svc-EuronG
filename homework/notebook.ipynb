{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "54fabd66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gzip\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    balanced_accuracy_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    confusion_matrix,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3b0b9080",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Config ----------\n",
    "TRAIN_PATH = \"../files/input/train_data.csv.zip\"\n",
    "TEST_PATH = \"../files/input/test_data.csv.zip\"\n",
    "MODEL_DIR = \"../files/models\"\n",
    "MODEL_FILE = os.path.join(MODEL_DIR, \"model.pkl.gz\")\n",
    "OUTPUT_DIR = \"../files/output\"\n",
    "METRICS_FILE = os.path.join(OUTPUT_DIR, \"metrics.json\")\n",
    "\n",
    "# Columnas categóricas a transformar\n",
    "CATEGORICAL_COLS = [\"SEX\", \"EDUCATION\", \"MARRIAGE\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d8f2b347",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Funciones de preparación ----------\n",
    "def load_raw_data(train_path: str, test_path: str):\n",
    "    \"\"\"Carga los CSVs comprimidos (zip)\"\"\"\n",
    "    train = pd.read_csv(train_path, compression=\"zip\")\n",
    "    test = pd.read_csv(test_path, compression=\"zip\")\n",
    "    return train, test\n",
    "\n",
    "\n",
    "def clean_dataset(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Limpieza según enunciado:\n",
    "    - Renombrar 'default payment next month' -> 'default'\n",
    "    - Eliminar 'ID'\n",
    "    - Para EDUCATION: valores >4 -> agrupar a 'others' (reemplazamos por 4)\n",
    "    - Eliminar registros con información no disponible (usamos np.nan y dropna)\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # Columna objetivo: renombrar si existe\n",
    "    if \"default payment next month\" in df.columns:\n",
    "        df = df.rename(columns={\"default payment next month\": \"default\"})\n",
    "\n",
    "    # Eliminar ID si existe\n",
    "    if \"ID\" in df.columns:\n",
    "        df = df.drop(columns=[\"ID\"])\n",
    "\n",
    "    # Marcar como missing algunos códigos inválidos (0 para EDUCATION o MARRIAGE)\n",
    "    # según ejemplo anterior: tratar 0 como missing y luego eliminar\n",
    "    if \"EDUCATION\" in df.columns:\n",
    "        df[\"EDUCATION\"] = df[\"EDUCATION\"].replace({0: np.nan})\n",
    "        # Agrupar valores mayores a 4 en 4 (others)\n",
    "        df.loc[df[\"EDUCATION\"].notna() & (df[\"EDUCATION\"] > 4), \"EDUCATION\"] = 4\n",
    "\n",
    "    if \"MARRIAGE\" in df.columns:\n",
    "        df[\"MARRIAGE\"] = df[\"MARRIAGE\"].replace({0: np.nan})\n",
    "\n",
    "    # Eliminar filas con NaN\n",
    "    df = df.dropna(axis=0).reset_index(drop=True)\n",
    "\n",
    "    # Asegurar tipos enteros para categorías\n",
    "    for col in [\"SEX\", \"EDUCATION\", \"MARRIAGE\"]:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype(int)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def split_xy(df: pd.DataFrame):\n",
    "    \"\"\"Separa X y y (target = 'default')\"\"\"\n",
    "    y = df[\"default\"].copy()\n",
    "    X = df.drop(columns=[\"default\"]).copy()\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e7389d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_pipeline():\n",
    "\n",
    "    col_transformer = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"cat\", OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\"), CATEGORICAL_COLS)\n",
    "        ],\n",
    "        remainder=\"passthrough\",\n",
    "    )\n",
    "\n",
    "    pipeline = Pipeline(\n",
    "        steps=[\n",
    "            (\"transformer\", col_transformer),\n",
    "            (\"scaler\", StandardScaler()),\n",
    "            (\"pca\", PCA(n_components=None)),\n",
    "            (\"selector\", SelectKBest(score_func=f_classif)),\n",
    "            (\"svc\", SVC()),\n",
    "        ]\n",
    "    )\n",
    "    return pipeline\n",
    "\n",
    "\n",
    "def run_grid_search(pipeline, X, y):\n",
    "    \"\"\"\n",
    "    GridSearchCV con cv=10, scoring balanced_accuracy.\n",
    "    Los hiperparámetros están elegidos para obtener resultados comparables\n",
    "    a los benchmarks del autograder.\n",
    "    \"\"\"\n",
    "    param_grid = {\n",
    "    \"svc__C\": [0.1, 1, 2, 5, 10],\n",
    "    \"svc__gamma\": [0.1, 0.1, 0.5, 1],\n",
    "    \"svc__kernel\": [\"rbf\"]\n",
    "    }\n",
    "\n",
    "    # Crear el objeto de validación estratificada\n",
    "    cv_strategy = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "\n",
    "    gs = GridSearchCV(\n",
    "        estimator=pipeline,\n",
    "        param_grid=param_grid,\n",
    "        scoring=\"balanced_accuracy\",\n",
    "        cv=10,\n",
    "        n_jobs=-1,\n",
    "        verbose=0,\n",
    "    )\n",
    "\n",
    "    gs.fit(X, y)\n",
    "    return gs\n",
    "\n",
    "\n",
    "# ---------- Métricas y guardado ----------\n",
    "def create_metrics_record(y_true, y_pred, dataset_name):\n",
    "    \"\"\"Crea el diccionario de métricas (tipo 'metrics')\"\"\"\n",
    "    return {\n",
    "        \"type\": \"metrics\",\n",
    "        \"dataset\": dataset_name,\n",
    "        \"precision\": float(accuracy_score(y_true=y_true, y_pred=y_pred)),\n",
    "        \"balanced_accuracy\": float(balanced_accuracy_score(y_true=y_true, y_pred=y_pred)),\n",
    "        \"recall\": float(recall_score(y_true=y_true, y_pred=y_pred)),\n",
    "        \"f1_score\": float(f1_score(y_true=y_true, y_pred=y_pred)),\n",
    "    }\n",
    "\n",
    "\n",
    "def create_cm_record(y_true, y_pred, dataset_name):\n",
    "    \"\"\"Crea el diccionario de la matriz de confusión con el formato pedido\"\"\"\n",
    "    mat = confusion_matrix(y_true=y_true, y_pred=y_pred)\n",
    "    # mat shape = (2,2) esperando clases 0 y 1\n",
    "    return {\n",
    "        \"type\": \"cm_matrix\",\n",
    "        \"dataset\": dataset_name,\n",
    "        \"true_0\": {\"predicted_0\": int(mat[0, 0]), \"predicted_1\": None},\n",
    "        \"true_1\": {\"predicted_0\": None, \"predicted_1\": int(mat[1, 1])},\n",
    "    }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1d1d4422",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_gzip(obj, path):\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    with gzip.open(path, \"wb\") as f:\n",
    "        pickle.dump(obj, f)\n",
    "\n",
    "\n",
    "def save_metrics_lines(metrics_list, path):\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for rec in metrics_list:\n",
    "            f.write(json.dumps(rec) + \"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4d503e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Cargar\n",
    "    train_raw, test_raw = load_raw_data(TRAIN_PATH, TEST_PATH)\n",
    "\n",
    "    # Limpiar\n",
    "    train = clean_dataset(train_raw)\n",
    "    test = clean_dataset(test_raw)\n",
    "\n",
    "    # Dividir X/y\n",
    "    X_train, y_train = split_xy(train)\n",
    "    X_test, y_test = split_xy(test)\n",
    "\n",
    "    # Pipeline\n",
    "    pipeline = build_pipeline()\n",
    "\n",
    "    # GridSearch / ajuste\n",
    "    grid = run_grid_search(pipeline, X_train, y_train)\n",
    "\n",
    "    # Guardar modelo comprimido\n",
    "    save_model_gzip(grid, MODEL_FILE)\n",
    "\n",
    "    # Predicciones sobre train/test\n",
    "    y_train_pred = grid.predict(X_train)\n",
    "    y_test_pred = grid.predict(X_test)\n",
    "\n",
    "    # Calcular métricas y matrices\n",
    "    metrics = []\n",
    "    metrics.append(create_metrics_record(y_train, y_train_pred, \"train\"))\n",
    "    metrics.append(create_metrics_record(y_test, y_test_pred, \"test\"))\n",
    "    metrics.append(create_cm_record(y_train, y_train_pred, \"train\"))\n",
    "    metrics.append(create_cm_record(y_test, y_test_pred, \"test\"))\n",
    "\n",
    "    # Guardar métricas (una línea JSON por registro)\n",
    "    save_metrics_lines(metrics, METRICS_FILE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff05a1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d42338",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb685a85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1068417a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
